{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wsl/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import NamedTuple, Generator\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, optim, nn\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from src.transformer import Transformer\n",
    "\n",
    "# https://www.kaggle.com/datasets/dhruvildave/en-fr-translation-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedSentences(NamedTuple):\n",
    "    fr: str\n",
    "    en: str\n",
    "\n",
    "class ListPairedSentences(NamedTuple):\n",
    "    fr: list[str]\n",
    "    en: list[str]\n",
    "\n",
    "    def __getitem__(self, index: int) -> PairedSentences:\n",
    "        return PairedSentences(self.fr[index], self.en[index])\n",
    "    \n",
    "class TrainingBatch(NamedTuple):\n",
    "    x: Tensor\n",
    "    y: Tensor\n",
    "    encoder_mask: Tensor | None\n",
    "    # no need for decoder mask, we'll set it after\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"TrainingBatch(x.shape={self.x.shape}, y.shape={self.y.shape}, encoder_mask.shape={self.encoder_mask.shape})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(tokenizer.vocab_size)\n",
    "\n",
    "def tokenize(text):\n",
    "    return tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=64,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True)\n",
    "\n",
    "def make_batch(paired_sentences: ListPairedSentences) -> TrainingBatch:\n",
    "    # Tokenize each sentence in the 'fr' and 'en' lists\n",
    "    fr_sentences = [tokenize(sentence) for sentence in paired_sentences.fr]\n",
    "    en_sentences = [tokenize(sentence) for sentence in paired_sentences.en]\n",
    "\n",
    "    # Stack tokenized tensors for batching\n",
    "    X_batch = torch.stack([x['input_ids'].squeeze(0) for x in fr_sentences])\n",
    "    Y_batch = torch.stack([y['input_ids'].squeeze(0) for y in en_sentences])\n",
    "\n",
    "    # Create encoder padding mask: 1 for real tokens, 0 for padding\n",
    "    encoder_mask = torch.stack([y['attention_mask'].squeeze(0) for y in fr_sentences]) \\\n",
    "        .unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    return TrainingBatch(\n",
    "        x=X_batch,\n",
    "        y=Y_batch,\n",
    "        encoder_mask=encoder_mask.to(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(csv_path: str, page: int, rows_per_page: int):\n",
    "    return pd.read_csv(csv_path, skiprows = 1 + page * rows_per_page, nrows=rows_per_page, header=None, names=[\"en\", \"fr\"])\n",
    "\n",
    "def make_generator(csv_path: str, rows_per_page: int) -> Generator[ListPairedSentences, None, None]:\n",
    "    i = 0\n",
    "    while True:\n",
    "        page = get_page(csv_path, i, rows_per_page)\n",
    "        fr_sentences = page[\"fr\"].to_list()\n",
    "        en_sentences = page[\"en\"].to_list()\n",
    "        yield ListPairedSentences(fr_sentences, en_sentences)\n",
    "        i += i\n",
    "\n",
    "def get_num_steps(csv_path: str, rows_per_page: int) -> int:\n",
    "    total_rows = sum(1 for _ in open(csv_path)) - 1 # minus one for header row\n",
    "    num_steps = (total_rows + rows_per_page - 1) // rows_per_page  # Round up\n",
    "    return num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "csv_path = \"data/en-fr.csv\"\n",
    "num_steps = get_num_steps(csv_path, batch_size)\n",
    "model = Transformer(vocab_size=tokenizer.vocab_size).to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    # Reset generator at the start of each epoch\n",
    "    progress_bar = tqdm(make_generator(csv_path, batch_size), total=num_steps)\n",
    "    progress_bar.set_description(f\"Epoch {epoch + 1}\")\n",
    "\n",
    "    for step, raw_batch in enumerate(progress_bar, start=1):\n",
    "        # Prepare data for the model\n",
    "        training_batch = make_batch(raw_batch)  # Converts batch to `TrainingBatch` format\n",
    "        input_ids = training_batch.x.to(\"cuda\")  # Source sentences token IDs (French)\n",
    "        target_ids = training_batch.y.to(\"cuda\")  # Target sentences token IDs (English)\n",
    "        encoder_mask = training_batch.encoder_mask.to(\"cuda\")  # Mask for encoder\n",
    "\n",
    "        # Shift target_ids for teacher forcing\n",
    "        decoder_input_ids = target_ids[:, :-1]  # All except last token as input\n",
    "        labels = target_ids[:, 1:]  # All except first token as target\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output_probs = model(input_ids, decoder_input_ids, encoder_mask=encoder_mask)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(\n",
    "            output_probs.view(-1, output_probs.size(-1)),\n",
    "            labels.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Optionally, print progress\n",
    "        progress_bar.set_postfix_str(f\"current loss : {loss.item():.4f} ;\"\n",
    "                                     f\"epoch loss : {epoch_loss / step:.4f}\")\n",
    "\n",
    "    # Print average loss per epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] completed, Average Loss: {epoch_loss / num_steps:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
