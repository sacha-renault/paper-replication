{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/dhruvildave/en-fr-translation-dataset\n",
    "\n",
    "from typing import NamedTuple, Generator\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, optim, nn\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizerFast\n",
    "from src.transformer import Transformer\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "VOCAB_FILE = \"vocab.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_char_tokenerizer(vocab_file):\n",
    "    import string\n",
    "    # all printable strings\n",
    "    vocab = {ch: idx for idx, ch in enumerate(\n",
    "        string.ascii_lowercase + string.digits + string.punctuation + \" \")}\n",
    "    v_length = len(vocab)\n",
    "\n",
    "    # update with special tokens\n",
    "    vocab.update({\n",
    "        \"[PAD]\": v_length,\n",
    "        \"[CLS]\": v_length + 1,\n",
    "        \"[SEP]\": v_length + 2,\n",
    "        \"[UNK]\": v_length + 3,\n",
    "        \"[MASK]\": v_length + 4,\n",
    "    })\n",
    "\n",
    "    # save in a file\n",
    "    with open(vocab_file, \"w\") as f:\n",
    "        for token, _ in sorted(vocab.items(), key=lambda item: item[1]):\n",
    "            f.write(token + \"\\n\")\n",
    "\n",
    "# load tokenerizer\n",
    "# create_char_tokenerizer(VOCAB_FILE)\n",
    "tokenizer = BertTokenizerFast(\n",
    "    vocab_file=VOCAB_FILE,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    "    clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedSentences(NamedTuple):\n",
    "    fr: str\n",
    "    en: str\n",
    "\n",
    "class ListPairedSentences(NamedTuple):\n",
    "    fr: list[str]\n",
    "    en: list[str]\n",
    "\n",
    "    def __getitem__(self, index: int) -> PairedSentences:\n",
    "        return PairedSentences(self.fr[index], self.en[index])\n",
    "\n",
    "class TrainingBatch(NamedTuple):\n",
    "    x: Tensor\n",
    "    y: Tensor\n",
    "    encoder_mask: Tensor | None\n",
    "    # no need for decoder mask, we'll set it after\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"TrainingBatch(x.shape={self.x.shape}, y.shape={self.y.shape}, encoder_mask.shape={self.encoder_mask.shape})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=200,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True)\n",
    "\n",
    "def make_batch(paired_sentences: ListPairedSentences) -> TrainingBatch:\n",
    "    # Tokenize each sentence in the 'fr' and 'en' lists\n",
    "    fr_sentences = [tokenize(sentence) for sentence in paired_sentences.fr]\n",
    "    en_sentences = [tokenize(sentence) for sentence in paired_sentences.en]\n",
    "\n",
    "    # Stack tokenized tensors for batching\n",
    "    X_batch = torch.stack([x['input_ids'].squeeze(0) for x in fr_sentences])\n",
    "    Y_batch = torch.stack([y['input_ids'].squeeze(0) for y in en_sentences])\n",
    "\n",
    "    # Create encoder padding mask: 1 for real tokens, 0 for padding\n",
    "    encoder_mask = torch.stack([y['attention_mask'].squeeze(0) for y in fr_sentences]) \\\n",
    "        .unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    return TrainingBatch(\n",
    "        x=X_batch,\n",
    "        y=Y_batch,\n",
    "        encoder_mask=encoder_mask.to(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(csv_path: str, page: int, rows_per_page: int):\n",
    "    return pd.read_csv(csv_path, skiprows = 1 + page * rows_per_page, nrows=rows_per_page, header=None, names=[\"en\", \"fr\"])\n",
    "\n",
    "def make_generator(csv_path: str, rows_per_page: int) -> Generator[ListPairedSentences, None, None]:\n",
    "    i = 0\n",
    "    while True:\n",
    "        page = get_page(csv_path, i, rows_per_page)\n",
    "        fr_sentences = page[\"fr\"].to_list()\n",
    "        en_sentences = page[\"en\"].to_list()\n",
    "        yield ListPairedSentences(fr_sentences, en_sentences)\n",
    "        i += 1\n",
    "\n",
    "\n",
    "def get_num_steps(csv_path: str, rows_per_page: int) -> int:\n",
    "    total_rows = sum(1 for _ in open(csv_path)) - 1 # minus one for header row\n",
    "    num_steps = (total_rows + rows_per_page - 1) // rows_per_page  # Round up\n",
    "    return num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "num_epochs = 1\n",
    "batch_size = 32\n",
    "csv_path = \"data/en-fr.csv\"\n",
    "num_steps = get_num_steps(csv_path, batch_size)\n",
    "model = Transformer(vocab_size=tokenizer.vocab_size, max_sequence_len=200).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    # Reset generator at the start of each epoch\n",
    "    progress_bar = tqdm(make_generator(csv_path, batch_size), total=num_steps)\n",
    "    progress_bar.set_description(f\"Epoch {epoch + 1}\")\n",
    "\n",
    "    for step, raw_batch in enumerate(progress_bar, start=1):\n",
    "        # Prepare data for the model\n",
    "        training_batch = make_batch(raw_batch)  # Converts batch to `TrainingBatch` format\n",
    "        input_ids = training_batch.x.to(DEVICE)  # Source sentences token IDs (French)\n",
    "        target_ids = training_batch.y.to(DEVICE)  # Target sentences token IDs (English)\n",
    "        encoder_mask = training_batch.encoder_mask.to(DEVICE)  # Mask for encoder\n",
    "\n",
    "        # Shift target_ids for teacher forcing\n",
    "        # TODO\n",
    "        # Have to fix this, this part masks a padding token most likely (or end of sequence in some\n",
    "        # special cases). Have to use mask of decoder to masks the input instead of removing it from\n",
    "        # the target_ids that has padding\n",
    "        decoder_input_ids = target_ids[:, :-1]\n",
    "        labels = target_ids[:, 1:]  # All except first token as target\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output_probs = model(input_ids, decoder_input_ids, encoder_mask=encoder_mask)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(\n",
    "            output_probs.view(-1, output_probs.size(-1)),\n",
    "            labels.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Optionally, print progress\n",
    "        progress_bar.set_postfix_str(f\"current loss : {loss.item():.4f} ; \"\n",
    "                                     f\"epoch loss : {epoch_loss / step:.4f}\")\n",
    "\n",
    "    # Print average loss per epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] completed, Average Loss: {epoch_loss / num_steps:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
