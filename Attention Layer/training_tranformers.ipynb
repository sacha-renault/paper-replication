{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/dhruvildave/en-fr-translation-dataset\n",
    "\n",
    "from typing import NamedTuple, Generator, Callable\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, optim, nn\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from src.transformer import Transformer\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedSentences(NamedTuple):\n",
    "    fr: str\n",
    "    en: str\n",
    "\n",
    "class ListPairedSentences(NamedTuple):\n",
    "    fr: list[str]\n",
    "    en: list[str]\n",
    "\n",
    "    def __getitem__(self, index: int) -> PairedSentences:\n",
    "        return PairedSentences(self.fr[index], self.en[index])\n",
    "\n",
    "class TrainingBatch(NamedTuple):\n",
    "    french: Tensor\n",
    "    english: Tensor\n",
    "    encoder_mask: Tensor | None\n",
    "    decoder_mask: Tensor | None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"TrainingBatch(x.shape={self.french.shape}, y.shape={self.english.shape}, encoder_mask.shape={self.encoder_mask.shape})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenerizer:\n",
    "    def __init__(self, sequence_length: int, name: str) -> None:\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "        self._seq_length = sequence_length\n",
    "\n",
    "    def tokenerize(self, text: str, padding=\"max_length\", truncation=True):\n",
    "        return self._tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self._seq_length,\n",
    "            padding=padding,\n",
    "            truncation=truncation)\n",
    "\n",
    "    def decode(self, token_ids: Tensor, **kwargs) -> str:\n",
    "        return self._tokenizer.decode(token_ids, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def tokenizer(self) -> AutoTokenizer:\n",
    "        return self._tokenizer\n",
    "\n",
    "    @property\n",
    "    def sequence_length(self) -> int:\n",
    "        return self._seq_length\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return self._tokenizer.vocab_size\n",
    "\n",
    "def make_batch(paired_sentences: ListPairedSentences, tokenizer: Tokenerizer) -> TrainingBatch:\n",
    "    # Tokenize each sentence in the 'fr' and 'en' lists\n",
    "    fr_sentences = [tokenizer.tokenerize(sentence) for sentence in paired_sentences.fr]\n",
    "    en_sentences = [tokenizer.tokenerize(sentence) for sentence in paired_sentences.en]\n",
    "\n",
    "    # Stack tokenized tensors for batching\n",
    "    X_batch = torch.stack([x['input_ids'].squeeze(0) for x in fr_sentences])\n",
    "    Y_batch = torch.stack([y['input_ids'].squeeze(0) for y in en_sentences])\n",
    "\n",
    "    # Create encoder and decoder padding mask: 1 for real tokens, 0 for padding\n",
    "    encoder_mask = torch.stack([x['attention_mask'].squeeze(0) for x in fr_sentences]) \\\n",
    "        .unsqueeze(1).unsqueeze(2)\n",
    "    decoder_mask = torch.stack([y['attention_mask'].squeeze(0) for y in en_sentences]) \\\n",
    "        .unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    return TrainingBatch(\n",
    "        french=X_batch,\n",
    "        english=Y_batch,\n",
    "        encoder_mask=encoder_mask.to(torch.float32),\n",
    "        decoder_mask=decoder_mask.to(torch.float32))\n",
    "\n",
    "def get_first_masked_token(mask: torch.Tensor) -> torch.IntTensor:\n",
    "    squeezed_mask = mask.squeeze(1).squeeze(1) # mask is shaped (bs, 1, 1, sequence_length)\n",
    "    first_masked_indices = (squeezed_mask == 0).int().argmax(dim=1)\n",
    "    first_masked_indices[squeezed_mask.sum(dim=1) == squeezed_mask.size(1)] = squeezed_mask.size(1)\n",
    "    return first_masked_indices\n",
    "\n",
    "def mask_last_token(current_mask: torch.Tensor) -> torch.Tensor:\n",
    "    first_masked_indices = get_first_masked_token(current_mask) # get the index of first masked token\n",
    "    last_token_indices = torch.clamp(first_masked_indices - 1, min=0) # to avoid negative indices\n",
    "    current_mask[torch.arange(current_mask.size(0)), 0, 0, last_token_indices] = 0 # set the last 1 token to 0\n",
    "    return current_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(csv_path: str, page: int, rows_per_page: int):\n",
    "    return pd.read_csv(csv_path, skiprows = 1 + page * rows_per_page, nrows=rows_per_page, header=None, names=[\"en\", \"fr\"])\n",
    "\n",
    "def make_generator(csv_path: str, rows_per_page: int) -> Generator[ListPairedSentences, None, None]:\n",
    "    i = 0\n",
    "    while True:\n",
    "        page = get_page(csv_path, i, rows_per_page)\n",
    "        fr_sentences = page[\"fr\"].to_list()\n",
    "        en_sentences = page[\"en\"].to_list()\n",
    "        yield ListPairedSentences(fr_sentences, en_sentences)\n",
    "        i += 1\n",
    "\n",
    "\n",
    "def get_num_steps(csv_path: str, rows_per_page: int) -> int:\n",
    "    total_rows = sum(1 for _ in open(csv_path)) - 1 # minus one for header row\n",
    "    num_steps = (total_rows + rows_per_page - 1) // rows_per_page  # Round up\n",
    "    return num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"data/en-fr.csv\"\n",
    "tokenizer = Tokenerizer(200, \"bert-base-uncased\")\n",
    "model = Transformer(vocab_size=tokenizer.vocab_size, max_sequence_len=tokenizer.sequence_length).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt and loss\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.tokenizer.pad_token_id).to(DEVICE)\n",
    "\n",
    "# training loop qty\n",
    "num_epochs = 1\n",
    "batch_size = 32\n",
    "num_steps = get_num_steps(csv_path, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    # Reset generator at the start of each epoch\n",
    "    progress_bar = tqdm(make_generator(csv_path, batch_size), total=num_steps)\n",
    "    progress_bar.set_description(f\"Epoch {epoch + 1}\")\n",
    "\n",
    "    for step, raw_batch in enumerate(progress_bar, start=1):\n",
    "        # Prepare data for the model\n",
    "        training_batch = make_batch(raw_batch, tokenizer)  # Converts batch to `TrainingBatch` format\n",
    "        input_ids = training_batch.french.to(DEVICE)  # Source sentences token IDs (French)\n",
    "        target_ids = training_batch.english.to(DEVICE)  # Target sentences token IDs (English)\n",
    "        encoder_mask = training_batch.encoder_mask.to(DEVICE)\n",
    "        decoder_mask = training_batch.decoder_mask.to(DEVICE)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output_probs = model(input_ids,\n",
    "                             target_ids,\n",
    "                             encoder_mask=encoder_mask,\n",
    "                             decoder_mask=mask_last_token(decoder_mask)) # mask last token for teacher forcing\n",
    "\n",
    "        # flatten target and outputprobs to compute cce loss\n",
    "        target_ids_flat = target_ids.view(-1)\n",
    "        output_probs_flat = output_probs.view(-1, output_probs.size(-1))\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(\n",
    "            output_probs_flat,\n",
    "            target_ids_flat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Optionally, print progress\n",
    "        progress_bar.set_postfix_str(f\"current loss : {loss.item():.4f} ; \"\n",
    "                                     f\"epoch loss : {epoch_loss / step:.4f}\")\n",
    "\n",
    "    # Print average loss per epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] completed, Average Loss: {epoch_loss / num_steps:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def infer(model: nn.Module, tokenerizer: Tokenerizer, french_sentence: str, max_length: int | None = None) -> str:\n",
    "    # first model as eval (we don't train here)\n",
    "    model.eval()\n",
    "    sequence_length = max_length if max_length else tokenerizer.sequence_length\n",
    "\n",
    "    # Tokenize the input sequence in french\n",
    "    tokens = tokenerizer.tokenerize(french_sentence)\n",
    "\n",
    "    # create the encoder input and mask\n",
    "    encoder_ids = tokens[\"input_ids\"].int().to(DEVICE)\n",
    "    encoder_mask = tokens[\"attention_mask\"].unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    # initialize decoder mask and input\n",
    "    decoder_mask = torch.zeros((1, 1, 1, tokenerizer.sequence_length)).to(DEVICE)\n",
    "    decoder_mask[0, 0, 0, 0] = 1 # unmask the start of sequence token\n",
    "    target_ids = torch.zeros((1, tokenerizer.sequence_length)).int().to(DEVICE)\n",
    "    target_ids[0, 0] = tokenerizer.tokenizer.cls_token_id # start of sequence\n",
    "\n",
    "    # loop to generate output ids\n",
    "    for idx in range(1, sequence_length):\n",
    "        output_probs: Tensor = model(\n",
    "            encoder_ids,\n",
    "            target_ids,\n",
    "            encoder_mask=encoder_mask,\n",
    "            decoder_mask=decoder_mask)\n",
    "\n",
    "        # select next token ID with the highest probability\n",
    "        next_token_id = output_probs.argmax(dim=-1)[0, idx]\n",
    "        target_ids[0, idx] = next_token_id\n",
    "        decoder_mask[0, 0, 0, idx] = 1 # unmask the generated token\n",
    "\n",
    "        # early stop when encounter sep_token_id\n",
    "        if next_token_id.item() == tokenerizer.tokenizer.sep_token_id:\n",
    "            break\n",
    "\n",
    "    return tokenerizer.decode(target_ids[0], skip_special_tokens=True)\n",
    "\n",
    "output = infer(model, tokenizer, \"ahah\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
