{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/dhruvildave/en-fr-translation-dataset\n",
    "\n",
    "from typing import NamedTuple, Generator\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, optim, nn\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from src.transformer import Transformer\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedSentences(NamedTuple):\n",
    "    fr: str\n",
    "    en: str\n",
    "\n",
    "class ListPairedSentences(NamedTuple):\n",
    "    fr: list[str]\n",
    "    en: list[str]\n",
    "\n",
    "    def __getitem__(self, index: int) -> PairedSentences:\n",
    "        return PairedSentences(self.fr[index], self.en[index])\n",
    "\n",
    "class TrainingBatch(NamedTuple):\n",
    "    x: Tensor\n",
    "    y: Tensor\n",
    "    encoder_mask: Tensor | None\n",
    "    decoder_mask: Tensor | None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"TrainingBatch(x.shape={self.x.shape}, y.shape={self.y.shape}, encoder_mask.shape={self.encoder_mask.shape})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize(text):\n",
    "    return tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=200,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True)\n",
    "\n",
    "def make_batch(paired_sentences: ListPairedSentences) -> TrainingBatch:\n",
    "    # Tokenize each sentence in the 'fr' and 'en' lists\n",
    "    fr_sentences = [tokenize(sentence) for sentence in paired_sentences.fr]\n",
    "    en_sentences = [tokenize(sentence) for sentence in paired_sentences.en]\n",
    "\n",
    "    # Stack tokenized tensors for batching\n",
    "    X_batch = torch.stack([x['input_ids'].squeeze(0) for x in fr_sentences])\n",
    "    Y_batch = torch.stack([y['input_ids'].squeeze(0) for y in en_sentences])\n",
    "\n",
    "    # Create encoder and decoder padding mask: 1 for real tokens, 0 for padding\n",
    "    encoder_mask = torch.stack([x['attention_mask'].squeeze(0) for x in fr_sentences]) \\\n",
    "        .unsqueeze(1).unsqueeze(2)\n",
    "    decoder_mask = torch.stack([y['attention_mask'].squeeze(0) for y in en_sentences]) \\\n",
    "        .unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    return TrainingBatch(\n",
    "        x=X_batch,\n",
    "        y=Y_batch,\n",
    "        encoder_mask=encoder_mask.to(torch.float32),\n",
    "        decoder_mask=decoder_mask.to(torch.float32))\n",
    "\n",
    "def get_first_masked_token(mask: torch.Tensor) -> torch.IntTensor:\n",
    "    squeezed_mask = mask.squeeze(1).squeeze(1) # mask is shaped (bs, 1, 1, sequence_length)\n",
    "    first_masked_indices = (squeezed_mask == 0).int().argmax(dim=1)\n",
    "    first_masked_indices[squeezed_mask.sum(dim=1) == squeezed_mask.size(1)] = squeezed_mask.size(1)\n",
    "    return first_masked_indices\n",
    "\n",
    "def mask_last_token(current_mask: torch.Tensor) -> torch.Tensor:\n",
    "    first_masked_indices = get_first_masked_token(current_mask) # get the index of first masked token\n",
    "    last_token_indices = torch.clamp(first_masked_indices - 1, min=0) # to avoid negative indices\n",
    "    current_mask[torch.arange(current_mask.size(0)), 0, 0, last_token_indices] = 0 # set the last 1 token to 0\n",
    "    return current_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(csv_path: str, page: int, rows_per_page: int):\n",
    "    return pd.read_csv(csv_path, skiprows = 1 + page * rows_per_page, nrows=rows_per_page, header=None, names=[\"en\", \"fr\"])\n",
    "\n",
    "def make_generator(csv_path: str, rows_per_page: int) -> Generator[ListPairedSentences, None, None]:\n",
    "    i = 0\n",
    "    while True:\n",
    "        page = get_page(csv_path, i, rows_per_page)\n",
    "        fr_sentences = page[\"fr\"].to_list()\n",
    "        en_sentences = page[\"en\"].to_list()\n",
    "        yield ListPairedSentences(fr_sentences, en_sentences)\n",
    "        i += 1\n",
    "\n",
    "\n",
    "def get_num_steps(csv_path: str, rows_per_page: int) -> int:\n",
    "    total_rows = sum(1 for _ in open(csv_path)) - 1 # minus one for header row\n",
    "    num_steps = (total_rows + rows_per_page - 1) // rows_per_page  # Round up\n",
    "    return num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "num_epochs = 1\n",
    "batch_size = 32\n",
    "csv_path = \"data/en-fr.csv\"\n",
    "num_steps = get_num_steps(csv_path, batch_size)\n",
    "model = Transformer(vocab_size=tokenizer.vocab_size, max_sequence_len=200).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    # Reset generator at the start of each epoch\n",
    "    progress_bar = tqdm(make_generator(csv_path, batch_size), total=num_steps)\n",
    "    progress_bar.set_description(f\"Epoch {epoch + 1}\")\n",
    "\n",
    "    for step, raw_batch in enumerate(progress_bar, start=1):\n",
    "        # Prepare data for the model\n",
    "        training_batch = make_batch(raw_batch)  # Converts batch to `TrainingBatch` format\n",
    "        input_ids = training_batch.x.to(DEVICE)  # Source sentences token IDs (French)\n",
    "        target_ids = training_batch.y.to(DEVICE)  # Target sentences token IDs (English)\n",
    "        encoder_mask = training_batch.encoder_mask.to(DEVICE)\n",
    "        decoder_mask = training_batch.decoder_mask.to(DEVICE)\n",
    "\n",
    "        # clone decoder_mask before it's modified\n",
    "        active_loss = decoder_mask.clone().int().view(-1)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output_probs = model(input_ids,\n",
    "                             target_ids,\n",
    "                             encoder_mask=encoder_mask,\n",
    "                             decoder_mask=mask_last_token(decoder_mask)) # mask last token for teacher forcing\n",
    "\n",
    "        # flatten target and outputprobs to compute cce loss\n",
    "        target_ids_flat = target_ids.view(-1)\n",
    "        output_probs_flat = output_probs.view(-1, output_probs.size(-1))\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(\n",
    "            output_probs_flat[active_loss],\n",
    "            target_ids_flat[active_loss])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Optionally, print progress\n",
    "        progress_bar.set_postfix_str(f\"current loss : {loss.item():.4f} ; \"\n",
    "                                     f\"epoch loss : {epoch_loss / step:.4f}\")\n",
    "\n",
    "    # Print average loss per epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] completed, Average Loss: {epoch_loss / num_steps:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
